{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ee81d5-7d45-41db-b520-4961e6f6af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass   \n",
    "from typing import List,Tuple,Optional,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95818e97-45f9-4e04-9437-ea2d72f7dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataclasses block will be used to store configuration like heads , channels,kernel \n",
    "# typing to specify what type of data each type should hold \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4170385-6b94-49a9-9f7b-341796f71978",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MSBlockConfig:\n",
    "    num_heads:int\n",
    "    input_channels:int\n",
    "    output_channels:int\n",
    "    kernel_q:List[int]\n",
    "    kernel_kv:List[int]\n",
    "    stride_q:List[int]\n",
    "    stride_kv:List[int]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031bfe43-5e47-4b23-aae8-f5813b1af7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prod(s:List[int])->int:\n",
    "  product=1\n",
    "  for v in s:\n",
    "   product*=v;\n",
    "  return product\n",
    "\n",
    "def _unsqueeze(x: torch.Tensor,target_dim:int,expand_dim:int):\n",
    "    tensor_dim=x.dim()\n",
    "    if tensor_dim==target_dim-1:\n",
    "        x=x.unsqueeze(expand_dim)\n",
    "\n",
    "    elif tensor_dim!=target_dim:\n",
    "        raise ValueError(f\"unsupported input dimension {x.shape}\")\n",
    "    return x,tensor_dim\n",
    "\n",
    "def _squeeze(x:torch.tensor,target_dim:int,expand_dim:int,tensor_dim:int):\n",
    "    if tensor_dim==target_dim-1:\n",
    "        x=x.squeeze(expand_dim)\n",
    "    return x   \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3187a-875c-429c-b50c-266dbe81df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  expand_dim is the index where new dimension will get added\n",
    "## squeeze and unsqueeze are used inside the pool block \n",
    "## squeeze and unsqueeze are paired operations first we unnsqueze then pool and squeeze it back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813ab2ee-4541-475e-a23f-0f958076072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pool: nn.Module,\n",
    "        norm: Optional[nn.Module],\n",
    "        activation: Optional[nn.Module] = None,\n",
    "        norm_before_pool: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "\n",
    "        layers = []\n",
    "        if norm is not None:\n",
    "            layers.append(norm)\n",
    "        if activation is not None:\n",
    "            layers.append(activation)\n",
    "\n",
    "    \n",
    "        if len(layers) > 0:\n",
    "            self.norm_act = nn.Sequential(*layers)\n",
    "        else:\n",
    "            self.norm_act = None\n",
    "\n",
    "        self.norm_before_pool = norm_before_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7f2cd-c8ce-412c-85e1-11a68bcaea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## super().__init__()\n",
    "## Initializes nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823383e4-6099-4290-82ff-3fdae6ced5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,x:torch.Tensor,thw: Tuple[int,int,int]):\n",
    "    x,tensor_dim=_unsqueeze(x,4,1)\n",
    "    class_token,x=torch.tensor_split(x,indices=(1,),dim=2)\n",
    "    x=x.transpose(2,3)\n",
    "    B,N,C=x.shape[:3]\n",
    "    x=x.reshape((B*N,C)+ thw)\n",
    "\n",
    "    if self.norm_before_pool and self.norm_act is not None:\n",
    "        x=self.norm_act(x)\n",
    "    x, thw_new = self.pool(x, thw)\n",
    "    T, H, W = thw_new\n",
    "    x=x.reshape(B,N,C,-1).transpose(2,3)\n",
    "    x=torch.cat((class_token,x),dim=2)\n",
    "\n",
    "    if not self.norm_before_pool and self.norm_act is not None:\n",
    "        x=self.norm_act(x)\n",
    "\n",
    "    x=_squeeze(x,4,1,tensor_dim)\n",
    "\n",
    "    return x,(T,H,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac703d-51a1-4555-8174-25c3fa132461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 dimension input is received by attention and popling converts it into 5 dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9671505b-04df-43ba-a32c-ac6abbf9ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: List[int],\n",
    "        embed_dim: int,\n",
    "        output_dim: int,\n",
    "        num_heads: int,\n",
    "        kernel_q: List[int],\n",
    "        kernel_kv: List[int],\n",
    "        stride_q: List[int],\n",
    "        stride_kv: List[int],\n",
    "        residual_pool: bool,\n",
    "        residual_with_cls_embed: bool,\n",
    "        rel_pos_embed: bool,\n",
    "        dropout: float = 0.0,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = output_dim // num_heads\n",
    "        self.scaler = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        self.residual_pool = residual_pool\n",
    "        self.residual_with_cls_embed = residual_with_cls_embed\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * output_dim)\n",
    "\n",
    "        layers: List[nn.Module] = [nn.Linear(output_dim, output_dim)]\n",
    "        if dropout > 0.0:\n",
    "            layers.append(nn.Dropout(dropout, inplace=True))\n",
    "        self.project = nn.Sequential(*layers)\n",
    "\n",
    "        #q pooling\n",
    "        self.pool_q: Optional[nn.Module] = None\n",
    "        if _prod(kernel_q) > 1 or _prod(stride_q) > 1:\n",
    "            padding_q = [k // 2 for k in kernel_q]\n",
    "            self.pool_q = Pool(\n",
    "                nn.Conv3d(\n",
    "                    self.head_dim,\n",
    "                    self.head_dim,\n",
    "                    kernel_q,\n",
    "                    stride=stride_q,\n",
    "                    padding=padding_q,\n",
    "                    groups=self.head_dim,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                norm_layer(self.head_dim),\n",
    "            )\n",
    "\n",
    "        #k,v pooling\n",
    "        self.pool_k: Optional[nn.Module] = None\n",
    "        self.pool_v: Optional[nn.Module] = None\n",
    "        if _prod(kernel_kv) > 1 or _prod(stride_kv) > 1:\n",
    "            padding_kv = [k // 2 for k in kernel_kv]\n",
    "            self.pool_k = Pool(\n",
    "                nn.Conv3d(\n",
    "                    self.head_dim,\n",
    "                    self.head_dim,\n",
    "                    kernel_kv,\n",
    "                    stride=stride_kv,\n",
    "                    padding=padding_kv,\n",
    "                    groups=self.head_dim,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                norm_layer(self.head_dim),\n",
    "            )\n",
    "            self.pool_v = Pool(\n",
    "                nn.Conv3d(\n",
    "                    self.head_dim,\n",
    "                    self.head_dim,\n",
    "                    kernel_kv,\n",
    "                    stride=stride_kv,\n",
    "                    padding=padding_kv,\n",
    "                    groups=self.head_dim,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                norm_layer(self.head_dim),\n",
    "            )\n",
    "\n",
    "        # relposembedng\n",
    "        self.rel_pos_h: Optional[nn.Parameter] = None\n",
    "        self.rel_pos_w: Optional[nn.Parameter] = None\n",
    "        self.rel_pos_t: Optional[nn.Parameter] = None\n",
    "\n",
    "        if rel_pos_embed:\n",
    "            size = max(input_size[1:])\n",
    "            q_size = size // stride_q[1] if len(stride_q) > 0 else size\n",
    "            kv_size = size // stride_kv[1] if len(stride_kv) > 0 else size\n",
    "\n",
    "            spatial_dim = 2 * max(q_size, kv_size) - 1\n",
    "            temporal_dim = 2 * input_size[0] - 1\n",
    "\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(spatial_dim, self.head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(spatial_dim, self.head_dim))\n",
    "            self.rel_pos_t = nn.Parameter(torch.zeros(temporal_dim, self.head_dim))\n",
    "\n",
    "            nn.init.trunc_normal_(self.rel_pos_h, std=0.02)\n",
    "            nn.init.trunc_normal_(self.rel_pos_w, std=0.02)\n",
    "            nn.init.trunc_normal_(self.rel_pos_t, std=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e67d425-4446-472f-888f-41ce46648d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: List[int],\n",
    "        cnf: MSBlockConfig,\n",
    "        residual_pool: bool,\n",
    "        residual_with_cls_embed: bool,\n",
    "        rel_pos_embed: bool,\n",
    "        proj_after_attn: bool,\n",
    "        dropout: float = 0.0,\n",
    "        stochastic_depth_prob: float = 0.0,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_after_attn = proj_after_attn\n",
    "\n",
    "        # Skip connection pooling\n",
    "        self.pool_skip: Optional[nn.Module] = None\n",
    "        if _prod(cnf.stride_q) > 1:\n",
    "            kernel_skip = [s + 1 if s > 1 else s for s in cnf.stride_q]\n",
    "            padding_skip = [k // 2 for k in kernel_skip]\n",
    "            self.pool_skip = Pool(\n",
    "                nn.MaxPool3d(kernel_skip, stride=cnf.stride_q, padding=padding_skip),\n",
    "                None,\n",
    "            )\n",
    "\n",
    "        attn_dim = cnf.output_channels if proj_after_attn else cnf.input_channels\n",
    "\n",
    "        self.norm1 = norm_layer(cnf.input_channels)\n",
    "        self.norm2 = norm_layer(attn_dim)\n",
    "\n",
    "        self.needs_transposal = isinstance(self.norm1, nn.BatchNorm1d)\n",
    "\n",
    "        self.attn = MultiscaleAttention(\n",
    "            input_size,\n",
    "            cnf.input_channels,\n",
    "            attn_dim,\n",
    "            cnf.num_heads,\n",
    "            kernel_q=cnf.kernel_q,\n",
    "            kernel_kv=cnf.kernel_kv,\n",
    "            stride_q=cnf.stride_q,\n",
    "            stride_kv=cnf.stride_kv,\n",
    "            residual_pool=residual_pool,\n",
    "            residual_with_cls_embed=residual_with_cls_embed,\n",
    "            rel_pos_embed=rel_pos_embed,\n",
    "            dropout=dropout,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            attn_dim,\n",
    "            [4 * attn_dim, cnf.output_channels],\n",
    "            activation_layer=nn.GELU,\n",
    "            dropout=dropout,\n",
    "            inplace=None,\n",
    "        )\n",
    "\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "\n",
    "        self.project: Optional[nn.Module] = None\n",
    "        if cnf.input_channels != cnf.output_channels:\n",
    "            self.project = nn.Linear(cnf.input_channels, cnf.output_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8c8c25-fb8c-4d9b-81a7-8526958b7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor, thw: Tuple[int, int, int]):\n",
    "    \n",
    "    if self.needs_transposal:       # before attention\n",
    "        x_norm1 = self.norm1(x.transpose(1, 2)).transpose(1, 2)\n",
    "    else:\n",
    "        x_norm1 = self.norm1(x)\n",
    "\n",
    "    \n",
    "    x_attn, thw_new = self.attn(x_norm1, thw)\n",
    "\n",
    "    \n",
    "    if self.project is not None and self.proj_after_attn:   # project before attention\n",
    "        x_res = self.project(x_norm1)\n",
    "    else:\n",
    "        x_res = x\n",
    "\n",
    "   \n",
    "    if self.pool_skip is not None:\n",
    "        x_skip = self.pool_skip(x, thw)[0]  ## to skip the cls token from pooling\n",
    "    else:\n",
    "        x_skip = x\n",
    "\n",
    "    \n",
    "    x = x_skip + self.stochastic_depth(x_attn)\n",
    "\n",
    "    \n",
    "    if self.needs_transposal:             # before mlp \n",
    "        x_norm2 = self.norm2(x.transpose(1, 2)).transpose(1, 2)\n",
    "    else:\n",
    "        x_norm2 = self.norm2(x)\n",
    "\n",
    "    \n",
    "    if self.project is not None and not self.proj_after_attn:  ## project before mlp after attention\n",
    "        x_res = self.project(x_norm2)\n",
    "    else:\n",
    "        x_res = x\n",
    "\n",
    "    x = x_res + self.stochastic_depth(self.mlp(x_norm2))\n",
    "\n",
    "    return x, thw_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4e27c5-6af6-410b-b426-a2f3faca7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size: int, spatial_size: Tuple[int, int], temporal_size: int, rel_pos_embed: bool) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_size = spatial_size\n",
    "        self.temporal_size = temporal_size\n",
    "\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.zeros(embed_size))\n",
    "\n",
    "        \n",
    "        self.spatial_pos: Optional[nn.Parameter] = None\n",
    "        self.temporal_pos: Optional[nn.Parameter] = None\n",
    "        self.class_pos: Optional[nn.Parameter] = None\n",
    "\n",
    "        if not rel_pos_embed:\n",
    "            self.spatial_pos = nn.Parameter(torch.zeros(spatial_size[0] * spatial_size[1], embed_size))\n",
    "            self.temporal_pos = nn.Parameter(torch.zeros(temporal_size, embed_size))\n",
    "            self.class_pos = nn.Parameter(torch.zeros(embed_size))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        class_token = self.class_token.expand(x.size(0), -1).unsqueeze(1)   # (b,c)\n",
    "        x = torch.cat((class_token, x), dim=1)  # added cls token \n",
    "\n",
    "        \n",
    "        if self.spatial_pos is not None and self.temporal_pos is not None and self.class_pos is not None:\n",
    "            hw_size, embed_size = self.spatial_pos.shape\n",
    "\n",
    "            \n",
    "            pos_embedding = torch.repeat_interleave(self.temporal_pos, hw_size, dim=0)\n",
    "\n",
    "            \n",
    "            pos_embedding += (\n",
    "                self.spatial_pos.unsqueeze(0)\n",
    "                .expand(self.temporal_size, -1, -1)\n",
    "                .reshape(-1, embed_size)\n",
    "            )\n",
    "\n",
    "            \n",
    "            pos_embedding = torch.cat((self.class_pos.unsqueeze(0), pos_embedding), dim=0).unsqueeze(0)  ## adding  cls token in positional embeddings\n",
    "  \n",
    "            x = x + pos_embedding\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "707f4a5e-8fd5-436c-8bbf-315b6e1f33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,               \n",
    "        input_size: List[int],            \n",
    "        embed_dim: int,                   \n",
    "        block_setting: List[MSBlockConfig],\n",
    "        num_classes: int,\n",
    "        rel_pos_embed: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "      \n",
    "        self.patch_embed = nn.Conv3d( \n",
    "            in_channels=input_channels, \n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=(3, 4, 4),\n",
    "            stride=(2, 4, 4),\n",
    "            padding=(1, 1, 1),  \n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        T, H, W = input_size\n",
    "\n",
    "        # no . of vectors to be made in t , h , w \n",
    "        self.embed_T = (T + 2*1 - 3) // 2 + 1    \n",
    "        self.embed_H = (H + 2*1 - 4) // 4 + 1    \n",
    "        self.embed_W = (W + 2*1 - 4) // 4 + 1    \n",
    "        self.embed_dim = embed_dim   // product of all three to tell the number of tokens \n",
    "\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            embed_size=embed_dim,\n",
    "            spatial_size=(self.embed_H, self.embed_W),\n",
    "            temporal_size=self.embed_T,\n",
    "            rel_pos_embed=rel_pos_embed,\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_size = [self.embed_T, self.embed_H, self.embed_W]  \n",
    "\n",
    "        for cnf in block_setting:\n",
    "\n",
    "            block = MultiscaleBlock(\n",
    "                input_size=current_size,\n",
    "                cnf=cnf,\n",
    "                residual_pool=True,\n",
    "                residual_with_cls_embed=True,\n",
    "                rel_pos_embed=rel_pos_embed,\n",
    "                proj_after_attn=True,\n",
    "                dropout=dropout,\n",
    "                stochastic_depth_prob=0.0,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "\n",
    "            self.blocks.append(block)\n",
    "\n",
    "            \n",
    "            t, h, w = current_size\n",
    "            if len(cnf.stride_q) > 0:\n",
    "                t = t // cnf.stride_q[0]\n",
    "                h = h // cnf.stride_q[1]\n",
    "                w = w // cnf.stride_q[2]\n",
    "\n",
    "            current_size = [t, h, w]\n",
    "\n",
    "       \n",
    "        self.final_size = current_size\n",
    "\n",
    "        self.norm=norm_layer(embed_dim)\n",
    "        self.head=nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim,num_classes)  # 96 dimensionss to 34 class classification \n",
    "        )\n",
    "\n",
    "\n",
    "        def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "            x=self.patch_embed(x)      ## converting videos into patches \n",
    "\n",
    "            B,C,T,H,W =x.shape\n",
    "            x=x.reshape(B,C,T*H*W).transpose(1,2)\n",
    "\n",
    "            x=self.pos_encoding(x)\n",
    "\n",
    "            thw=(T,H,W)\n",
    "\n",
    "            for block in self.blocks:\n",
    "                x,thw = block(x,thw)\n",
    "\n",
    "            x=self.norm(x)\n",
    "\n",
    "            cls_token=x[:,0]\n",
    "\n",
    "            out=self.head(cls_token)\n",
    "\n",
    "            return out\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc319d32-57c7-4688-b0ca-cbbf6340f13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de065c-5b42-4562-a80d-6184409101c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
